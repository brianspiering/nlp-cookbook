{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21adaf05",
   "metadata": {},
   "source": [
    "Cluster word embeddings\n",
    "=====\n",
    "\n",
    "Given a collection of text documents, learn embeddings and then cluster based on embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf654b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   gensim.models   import Word2Vec\n",
    "from   gensim.utils    import tokenize\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "from   sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ba575",
   "metadata": {},
   "source": [
    "Define the corpus\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce60bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"I like an apple for breakfast\",\n",
    "        \"My kitten is fluffy\",\n",
    "        \"My breakfast banana and kale smoothie was delicious \",\n",
    "        \"My kitten loves kale\",\n",
    "        \"Banana and apple go well together for breakfast\",\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759a55d",
   "metadata": {},
   "source": [
    "Learn the embeddings\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba02359",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenized = [list(tokenize(doc)) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "933dc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word embedding model \n",
    "model = Word2Vec(docs_tokenized, \n",
    "                 window=2,        # window refers to the size of our context window\n",
    "                 min_count=1,     # typicially you would drop hapexes (words that only appear once)\n",
    "                 sg=True          # sg means that we are using the Skip-gram architecture\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e36487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each document as the mean of its word embeddings \n",
    "docs_vectorized = []\n",
    "\n",
    "for tokens in docs_tokenized:\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            try:\n",
    "                vectors.append(model.wv[token])\n",
    "            except KeyError:\n",
    "                continue\n",
    "    if vectors:\n",
    "        vectors = np.asarray(vectors)\n",
    "        avg_vec = vectors.mean(axis=0)\n",
    "        docs_vectorized.append(avg_vec)\n",
    "    else:\n",
    "        docs_vectorized.append(np.zeros(model.vector_size))\n",
    "        \n",
    "docs_vectorized = np.asarray(docs_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d0ecbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'an', 'apple', 'for', 'breakfast']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa907484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.96934818e-04,  8.70561169e-04, -7.12622132e-04,  2.30771140e-03,\n",
       "       -7.14178430e-04, -2.21090019e-03, -6.35348028e-04,  4.93225083e-03,\n",
       "       -1.22091989e-03, -2.97745760e-03,  4.25004028e-03, -1.20351440e-03,\n",
       "        3.50226625e-03,  1.67101083e-04,  2.43561692e-03, -3.18139646e-04,\n",
       "        3.07628629e-03, -4.86158446e-04, -3.61174531e-03, -3.82510759e-03,\n",
       "       -1.18583594e-04,  5.52570156e-04,  1.03268260e-03,  8.27280397e-04,\n",
       "        7.63246731e-04, -1.82972802e-03,  1.59627118e-03,  3.10474425e-03,\n",
       "       -2.51456420e-03, -8.15274718e-04,  4.85844910e-04, -2.67807674e-03,\n",
       "        1.50517502e-03,  1.26628217e-03, -2.05867947e-03,  2.96539883e-03,\n",
       "        3.97296576e-03,  6.21286861e-04, -1.85975258e-03,  1.71868049e-03,\n",
       "       -1.01122679e-03,  2.55969004e-03, -3.58969974e-03,  1.74651609e-03,\n",
       "        3.78081226e-04, -1.22238265e-03, -2.65470799e-03,  8.55162449e-04,\n",
       "       -3.40040628e-04,  3.22205690e-03,  1.47453204e-04,  3.32787633e-04,\n",
       "       -4.80977027e-03, -1.56384998e-03, -8.31723853e-04, -1.93585095e-03,\n",
       "       -1.27085717e-04, -1.49722258e-03, -1.20625715e-04, -1.47699539e-04,\n",
       "       -1.56194484e-03,  2.29643448e-03,  4.94283997e-03, -2.53691385e-03,\n",
       "       -4.76894993e-03,  2.18207599e-03, -1.64860312e-03, -2.59201508e-04,\n",
       "       -2.24487201e-04, -2.52014864e-03,  4.40488057e-03,  3.97794135e-03,\n",
       "       -2.15601735e-03,  3.50722432e-04,  1.42679445e-03,  2.76835915e-03,\n",
       "        3.69097199e-03,  3.46697896e-04, -2.06801901e-03, -4.32291999e-05,\n",
       "       -1.74552595e-04,  1.27404940e-03,  8.26188596e-04,  3.58117372e-03,\n",
       "       -3.56443785e-03,  2.45363946e-04,  7.79634211e-05, -7.44083722e-04,\n",
       "        1.89767720e-03,  2.51848111e-03, -3.04175331e-03,  3.00655048e-03,\n",
       "        2.57134973e-03,  2.51931348e-03,  6.36819517e-03, -3.03531215e-05,\n",
       "        9.64536332e-04, -2.54798005e-03,  1.72654947e-03, -2.23265449e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectorized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cd1c0",
   "metadata": {},
   "source": [
    "Learn clusters\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61d2c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster each vectorized document \n",
    "kmeans = KMeans(n_clusters=2, n_init='auto')\n",
    "kmeans.fit(docs_vectorized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e389e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My kitten is fluffy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My kitten loves kale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like an apple for breakfast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My breakfast banana and kale smoothie was deli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banana and apple go well together for breakfast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cluster\n",
       "1                                My kitten is fluffy        0\n",
       "3                               My kitten loves kale        0\n",
       "0                      I like an apple for breakfast        1\n",
       "2  My breakfast banana and kale smoothie was deli...        1\n",
       "4    Banana and apple go well together for breakfast        1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\" \".join(text) for text in docs_tokenized],\n",
    "    \"cluster\": kmeans.labels_\n",
    "}).sort_values(by=['cluster'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a1323",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/\n",
    "- https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
